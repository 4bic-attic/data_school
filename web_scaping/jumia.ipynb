{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-8df46f8f6cba>, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-8df46f8f6cba>\"\u001b[0;36m, line \u001b[0;32m60\u001b[0m\n\u001b[0;31m    link_regex = url/tenders/\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import urllib2 as ur\n",
    "import re\n",
    "def download(url, num_retries): #\n",
    "    # catch these exceptions:\n",
    "    print 'Downloading: ', url\n",
    "    user_agent='dcloud'\n",
    "    headers = {'User-agent': user_agent}\n",
    "    request = ur.Request(url, headers=headers)\n",
    "    try:\n",
    "        html = ur.urlopen(url).read()\n",
    "    except ur.URLError as e:\n",
    "        print \"Download Error : \", e.reason\n",
    "        html = None\n",
    "        # retries the 5xx errors\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code <= 600:\n",
    "                # recursively retry 5xx HTTP errors\n",
    "                return download(url, num_retries-1)\n",
    "\n",
    "    return html\n",
    "\n",
    "# Sitemap crawler\n",
    "def crawl_sitemap(url):\n",
    "    # download sitemap file\n",
    "    sitemap = download(url, num_retries)\n",
    "    # extract links\n",
    "    links = re.findall('<loc>(.*?)</loc>', sitemap)\n",
    "    # download each link\n",
    "    for link in links:\n",
    "        html = download(link)\n",
    "\n",
    "# crawl links\n",
    "def link_crawler(seed_url, link_regex):\n",
    "    #  Crawl from the given seed URL following links matched by link_regex\n",
    "    crawl_queue = [seed_url]\n",
    "\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        html = download(url)\n",
    "        # filter for links matching regular expression\n",
    "        for links in get_links(html):\n",
    "            if re.match(link_regex, link):\n",
    "                crawl_queue.append(link)\n",
    "\n",
    "def get_links(html):\n",
    "    # return list of links available\n",
    "    # extract all links from webpage\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']',\n",
    "                    re.IGNORECASE)\n",
    "    # list links from webpage\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # url = 'http://httpstat.us/500'\n",
    "    num_retries = 5\n",
    "    # url= 'http://supplier.treasury.go.ke/site/tenders.go/index.php'\n",
    "    url = 'https://www.jumia.co.ke'\n",
    "    link_regex = url/tenders/\n",
    "    download(url, num_retries)\n",
    "    crawl_sitemap(url)\n",
    "    link_crawler(seed_url, link_regex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
